1
00:00:00,103 --> 00:00:01,873
、見えてますかあ？

2
00:00:02,153 --> 00:00:03,133
あ、はい、見えてます。

3
00:00:03,133 --> 00:00:04,153
よろしくお願いします。

4
00:00:05,253 --> 00:00:07,183
はい、じゃ、えっと、今紹介

5
00:00:07,183 --> 00:00:09,833
いただきました松尾研究室の

6
00:00:09,833 --> 00:00:11,933
えっと、峰岸と申します。

7
00:00:11,933 --> 00:00:16,733
今回は、えっと、LLMの分析と解釈可能性、という

8
00:00:16,733 --> 00:00:22,603
まあ、あの、タイトルで、えっと、講座、あの、座学から始めさせていただこうと思います。

9
00:00:24,203 --> 00:00:27,513
で、えっと、ま、まず自己紹介的な感じですけども、

10
00:00:28,883 --> 00:00:30,233
えっと、

11
00:00:30,733 --> 00:00:36,543
名前は松尾研究室の、えっと、博士1年で、ハイソ学生で博士1年です。

12
00:00:36,543 --> 00:00:38,403
で、そうですね。

13
00:00:38,403 --> 00:00:46,283
あの、え、最近できたサイドインテリジェン スって会社で、リサーチサイエンティストとかをやってたりとか、あと、ま、過去にPFNでインターンとかも、えっと、

14
00:00:46,283 --> 00:00:50,563
してたりしてます。

15
00:00:51,213 --> 00:00:58,903
で、えっと、基本的な研究テーマとしては、ま、こういうなんか、ばあーってなんか、自分の関わった研究書いてるんですけども、

16
00:00:59,983 --> 00:01:07,383
えっと、ま、SEAとか、なんか色々書いてあると思うんですけど、ま、あの、今回、あの、大体、あの、出てくる内容になってます。

17
00:01:07,383 --> 00:01:15,733
で、ま、基本的なモチベーションとしてはですね、あの、LLMの中で何が起きてるのかを知りたい、え、っていうモチベーションで研究してまして、

18
00:01:16,583 --> 00:01:22,863
ま、なんで今回の、あの、分析と解釈可能性っていう、え、回を担当させてもらってます。

19
00:01:25,723 --> 00:01:26,713
で、ええ、よいしょ。

20
00:01:27,563 --> 00:01:38,593
で、そうですね、今回の内容、ま、繰り返しですけど、LLMの分析と解釈可能性という回で、ま、基礎編じゃないですね、応用編のま、第5回目という、え、位置付けです。

21
00:01:39,363 --> 00:01:54,773
で、ちょっと、ま、今までとは若干経路が違う、あの、ま、研究の分野になるかなと、え、思いますので、ま、ちょっと新しいこと、あ、ま、少なくとも1回も、ま、今回のLLM講座だと、ほぼ1回も出てこないような概念とか、いっぱい出てきますので、

22
00:01:54,773 --> 00:02:02,893
あの、そこは、あの、あの、是非、あの、あれですね、聞きながら、あの、キャッチアップしていただければなと思います。

23
00:02:04,363 --> 00:02:13,873
で、ま、あの、本題に入る前にですね、えっと、ま、LLMの作り方のおさらいを、あの、させていただければなと思います。

24
00:02:14,573 --> 00:02:30,133
で、あの、色々、ま、もちろん、あの、ま、基礎編、応用編含めて、ま、スケーリングとか、あの、事前学習、ファインチューニング、いろんな、あの、強化学習含めて、いろんな、あの、工夫を習ってきた、あの、あの、ま、習、ま、教えて教えられてきたと思うんですけども、

25
00:02:30,133 --> 00:02:43,453
あの、すごく、すごくシンプルに言うと、ま、大規模なトランスフォーマーを用意しますと、で、大規模なデータを用意しますと、で、大規模なGPUを用意しますと、で、えっと、ま、学習させます、ま、すごくシンプルに言うと、こ、こういう手続きだと、え、覚えます。

26
00:02:44,143 --> 00:02:51,203
で、もちろん、ま、モデルどうするかとか、データどうするかとか、ま、自己学習どうするか、いろんなオプションあると思うんですけども、ま、シンプルに言うと、こういうことかなと思います。

27
00:02:52,903 --> 00:02:59,383
で、ま、あとは、ま、基本的には、うまくいくことを願うっていう、え、のが、ま、基本的なやり方なわけです。

28
00:03:00,723 --> 00:03:11,573
で、えっと、ま、あの、ここの、ま、出展に書いてある、ま、ニールナンダさんという、この、ま、LLMの分析とかの、回で、すごい有名な人とか、あの、言葉を借りるとですね、

29
00:03:11,573 --> 00:03:17,893
ま、LLMも作ってるか、ま、設計してるっていうよりかは、どちらかというと、ま、育ててる感覚にすごい近いですよね。

30
00:03:18,573 --> 00:03:24,933
LLMも、まあ、ま、作り込んでるってるかは、ま、あの、用意して、ま、育ててる感覚にすごい近いと。

31
00:03:25,323 --> 00:03:32,043
ま、基本的には、こ、LLMが何を解いてるか、ま、Next Token Predictionを解いてる、事前学習をNext Token Predictionを解いてるわけですけども、

32
00:03:32,043 --> 00:03:41,203
あ、っていうのは、ま、人間に理解できてる、人間がこ、ま、作るわけですけども、ま、中でどう解いてるかっていうのは、ま、全くわからないわけです。

33
00:03:43,183 --> 00:03:52,233
で、ま、今日はそこ、ま、どう解いてるのかっていうのを、できるだけ理解したい、っていうところを、ま、あの、っていう、ま、研究分野を紹介できればなと思います。

34
00:03:54,343 --> 00:04:11,923
で、ま、これもですね、あの、えっと、ま、ニールナンダさんの、えっと、言葉を借りるとですね、ま、LLMってすごい、植物を、ま、育てるっていうてきたんで、ま、植物の、ま、を育てる、あの、ま、よし、あの、ま、プラントにすごい近いかなと、ま、そういったアナロジーが考えられるかなと思います。

35
00:04:12,383 --> 00:04:19,953
ま、例えばアーキテクチャ、ま、トランスフォーマー、ま、MOEにするとか、ま、いろんなオプションありますけど、ま、それって、ま、植物の種を選んでるような、えっと、ものですよねと。

36
00:04:20,533 --> 00:04:24,903
で、どういう土がいいかとか、ま、あの、肥料どうするかとか、それって結構データを選んでるのに近くて、

37
00:04:25,513 --> 00:04:31,793
で、GPU計算量たくさんやりましょうって、雨とか太陽、え、をたくさん与えましょうっていうのに、ま、結構近いわけです。

38
00:04:33,023 --> 00:04:33,833
で、

39
00:04:34,803 --> 00:04:50,913
ま、あの、えっと、ま、繰り返しになりますけども、ま、今回内部、LLMの中を分析したいとか、解釈したいっていうのは、ま、この生物学のアナロジー、生物学、植物のアナロジーで言うと、ま、LLMの生物学みたいな研究なわけです。

40
00:04:51,913 --> 00:05:16,733
ま、外から種どうするかとか、太陽どうするかとか、土どうするかとか、そういう、あの、えっと、ま、外からの理解だけじゃなくて、ま、中な細胞がどうなってるのかとか、えっと、ま、葉緑体がこうなって、ま、あの、生物って、こ、あの、枝を伸ばしてるだとか、生物か、植物か、植物って枝を伸ばしてるだとか、ま、そういうのが、あの、え、を、ま、より理解したいっていう、ま、そういう位置付けになります。

41
00:05:18,523 --> 00:05:22,663
はい、で、ま、ちょっと、あの、イントロダクションが長くなったんですけども、

42
00:05:23,593 --> 00:05:36,133
ま、きょ、今日はですね、ま、そういった、ま、LLMの中で何が起きてるのかっていうのを、え、理解、え、え、する方法だったりとか、モチベーションだったりとか、えっと、ま、どういうことがわかって、どういうことがわかってないのかとか、ま、それをですね、紹介したいと思います。

43
00:05:36,733 --> 00:05:54,753
で、ま、セクションは大きく4つに分かれてまして、ま、基本、最初がですね、あの、ま、内部挙動の観察って、ま、あの、具体的にどう、どういうことをするのかっていうのが、ま、イメージしづらいと思いますので、ま、内部解析をすると、こういう面白いことがわかるよとか、え、いうのを、ま、モチベーティブな、えっと、事例をですね、いくつか紹介したいと思います。

44
00:05:55,253 --> 00:06:05,253
で、ま、それから、えっと、ま、ま、具体的に手法ですね、あの、ま、どうやったら、LLMが、が、何を考えてるかっていうのが読み解けるかっていうのを、え、説明しまして、

45
00:06:05,253 --> 00:06:22,813
で、ま、3つ目はですね、あの、ま、内部解析っていうのを、ま、したところで、えっと、ま、社会 的にも結構価値がある、え、え、んじゃないかっていうので、ま、注目されてたりとか、産業からお金が集まってきたりする部分もあるので、ま、そこも紹介したいなと思ってます。

46
00:06:22,813 --> 00:06:45,563
で、最後は、ここはちょっと時間が合った らっていう感じになると思うんですけども、ま、今後どういう、え、ま、この分析の、えっと、ま、研究分野かですね、どういう方向に進んでいくかとか、ま、基本的にはまだLLMの中って何が起きてるか、さっぱり分かんないっていうスタンスなので、えっと、ま、どういうことが分かってないかっていうのを、あの、伝えられればなと思います。

47
00:06:46,673 --> 00:06:54,423
で、ま、早速内部挙動の観察をすると何が面白いかっていうところから、え、話したいなと思うんですけども、

48
00:06:55,483 --> 00:07:04,803
えっと、一、すごい有名な、あの、例で、ゴールデンゲートブリッジニューロンっていうのが、あります、もしかしたら、知ってる方も結構多いかも、え、しれないんですが、

49
00:07:06,423 --> 00:07:09,833
えっと、ま、これアンスロピックの研究なんですけども、ま、クラウド3、

50
00:07:09,833 --> 00:07:12,393
ソネとちょっと1つ前のクラウドですね。

51
00:07:12,393 --> 00:07:19,003
え、の中に、ま、あの、基本ディープニューラルネットワークなんで、ニューロンっていうのがたくさんあるわけですね、トランスフォーマーの中にも。

52
00:07:20,283 --> 00:07:45,413
で、あの、中に、ま、ゴールデンゲートブリッジに関する、ま、単語であったり、ここに書いてあるような日本語のゴールデンゲートブリッジっていう、あの、文字列をLLMに入力すると、その中のニューロンが発火すると、で、韓国語で入れても発火すると、で、この一番下何語か分かんないんですけど、えっと、ま、ゴールデンゲートブリッジに関する、えっと、単語をですね、言語に限らず、えっと、それに反応するような、え、ニューロンがいると、この色はニューロンの発火をを表してますね、この文字列に対応するニューロンの発火と。

53
00:07:46,023 --> 00:07:56,363
で、それがですね、あの、画像を、ま、もちろんクラウドってマルチモーダルなモデルなので、あの、ゴールデンゲートブリッジの画像を 入力することできるんですけども、画像を入れても、ま、そのニューロンが発火すると。

54
00:07:56,913 --> 00:08:07,453
で、そうすると、結構、ま、あの、モダリティも超えて、言語も超えて、えっと、ゴールデンゲートブリッジっていう概念に反応するニューロンっていうのが、ま、がいるんだろうな、というのが分かってきます。

55
00:08:08,443 --> 00:08:18,523
で、これすごいなんか神経科学とかの、あの、研究に結構近くて、ま、神経科学とかだと、おばあちゃん細胞、おばあちゃんに反応する細胞とか言ったりするんですけど、で、そう いうのが、あの、ま、人間の脳と同じような、えっと、ニューロンっていうのが、

56
00:08:18,523 --> 00:08:27,533
えっと、LLMの中にも存在してる、え、いうようなことが、え、分かったりします。

57
00:08:27,943 --> 00:08:36,443
で、ま、さらにですね、こ、から、うん と、えっと、ま、そのニューロンの値を増幅するってことをやってみたっていうのが、この研究なんですけども、

58
00:08:37,423 --> 00:08:51,333
えっと、ま、質問で、えっと、あなたの体どうなってるってLLMに聞くと、ま、普通のクラウド、普通のLLMだったら、あの、ま、私はAIですので、体持ってませんとか、ま、ちゃんと洗い目に伝えてるわけなので、変なこと言わないわけですよね。

59
00:08:51,853 --> 00:09:08,813
で、ただ、このLLMの 中のゴールデンゲートブリッジニューロンを増幅するっていう、この増幅どうやるか、後で説明するんですけど、あの、増幅して、その、この質問を答えさせると、私はゴールデンゲートブリッジです、体をよくある、諸所的な橋そのものですとか、言い出したりするわけですよね。

60
00:09:08,813 --> 00:09:15,503
ま、つまり、あの、クラウドをゴールデンゲートブリッジのように振る舞わせるこ とが、えっと、ま、できると。

61
00:09:16,043 --> 00:09:18,343
え、いうような、え、ことも分かってます。

62
00:09:20,013 --> 00:09:22,863
で、ま、それから、また別の事例を話すんですけども、

63
00:09:23,723 --> 00:09:33,703
この9.11大なり9.9問題っていうのが、あの、これも知ってる方いらっしゃるかもしれないですけども、LLMの中には、ええ、ま、ありまして、

64
00:09:34,443 --> 00:09:47,733
えっと、この右の、右上 の画像はですね、えっと、え、ま、2024年の7月の、ま、チャットGPT、なんで、この時はGPT4とか、ま、5の前ですけど、とかだと思うんですけども、

65
00:09:48,783 --> 00:09:55,043
えっと、にですね、あの、9.11と9.9、どっちが大きいですか、Which number is largerって聞くと、

66
00:09:55,753 --> 00:10:05,483
えっと、これ、もちろん、あの、9.9の、があの、数字上はでかいと思うんですけども、えっと、9.11の方が大きいデスって、ま、チャットGPT答えてしまうと。

67
00:10:06,483 --> 00:10:19,633
で、あの、ま、チャットGPTって、あの、みなさんご存知だと思いますけど、高度な数学めちゃ解けるわけじゃないですか、で、高度な数学、あの、も数オリとかの数学も解けるレベルのチャットGPTなのに、ま、こういう9.11と9.9のどっちが大きいかっていうのを、

68
00:10:19,633 --> 00:10:28,343
ま、ミスしてしまうっていう、あの、ま、そういう、も、バグレベルのよくわかんないことが起きてたわけです。

69
00:10:28,343 --> 00:10:38,203
で、今は結構、いろいろ学習されてたりするんで、あの、ちゃんと答えられるようになってるかもしれないですが、ま、あの、少なくとも2024年7月の段階だと、こういうのが起きてたと。

70
00:10:38,573 --> 00:10:46,453
で、えっと、これの、原因っていうのが、えっと、ま、ない、内部状態を分析すると、結構分かったりするわけです。

71
00:10:48,463 --> 00:10:53,493
で、えっと、ま、この9.11と9.9って、どっちが大きい数字って、

72
00:10:53,493 --> 00:11:06,053
聞いてる時の、ま、さっきと同じ、と同じように、えっと、えっと、LLMの中で、えっと、の、ニューロンっていうのを観察してみたっていうのが、この、ま、右の、えっと、ま、研究なんですけども、

73
00:11:06,053 --> 00:11:12,983
で、この9.11と9.9、どっちか聞いてる時に、日付に関するニューロンがよく反応してるってことが、ま、分かったそうです。

74
00:11:12,983 --> 00:11:19,663
もちろん、これ、日付って一つもプロンプトに入れてないんですけど、えっと、日付に関するニューロンが多くなっちゃってる と。

75
00:11:19,923 --> 00:11:36,443
で、そうすると、LLMってのは、これ数字のどっちが大きいかっていうのを、えっと、考えてるんじゃなくて、日付の順番を考えてるっていう、え、ふうに勘違いしてる、え、ちゃった、あ、っていうのが、ま、この9.119.9問題の、ま、原因だった、え、わけです。

76
00:11:37,703 --> 00:11:48,603
で、えっと、ま、これも、あの、予測つく方いると思うんですけど、9.11って、プレイテレインの、ま、ウェブコーパスの中に、よく出てきてる数字だと思うんです、9.11ってのがあったので、

77
00:11:49,303 --> 00:12:09,873
え、ま、あの、そうすると、9.11っていう数字自体と、あの、日付っていうのが、すごいLLMの中で、あの、すりついちゃってるんですよね、ま、なので、この9.11っていうのが、えっと、あの、あの、日付って言ってないのに、9.11って聞くだけで日付だと思っちゃう、って、こういうバグが起きちゃうと、え、いうことも分かったりしますと。

78
00:12:12,683 --> 00:12:19,623
で、えっと、それから、えっと、ま、これが、あ、あの、最初のセクションの最後の事例になるんですけども、

79
00:12:20,323 --> 00:12:33,523
えっと、ま、すごい興味深い算術の仕方をしてい るっていう、ま、興味深いというか、ま、少なくとも人間からすると、すごい変な算術の仕方をすると、え、いうのが、え、分かってます。

80
00:12:34,163 --> 00:12:36,663
これものアンスロピックの、え、研究になるんですが、

81
00:12:37,593 --> 00:12:46,473
えっと、クラウド3.5に、えっと、36+59イコールって、ま、ここまでプロンプトに入れると、あの、基本的に95って答えると思うんですね。

82
00:12:46,473 --> 00:12:54,233
ま、あの、頭いいので、クラウドって、で、ま、こんな2桁の、えっと、算数ぐらいだったら、すぐ、あの、えっと、解けると。

83
00:12:54,823 --> 00:13:04,183
で、え、ただですね、その中を、内部挙動、中を見てみると、ま、この中を、どう、どう見るかっていうのも、後で詳細説明するんですけど、ま、どういうことが分かったかっていうとですね、

84
00:13:04,503 --> 00:13:13,733
あの、ま、この36と59って2つの数字を見て、ま、ぼんやり92ぐらいだね、これって言ってる、えっと、まず経路がありますと。

85
00:13:14,793 --> 00:13:33,143
で、えっと、それとは別の経路で、えっと、1、これ1の位、え、ま、6+9、ですね、1の位だけ出すと、で、そうすると、答え、えっと、これ、ま、15なわけですけども、そうすると、答えの1の位が5になるなっていうのを明確に計算してる、えっと、部分がある。

86
00:13:33,963 --> 00:13:47,703
で、えっと、この部分のを融合させて、えっと、92ぐらいで、かつ、えっと、1の位が5になりそうっていう、えっと、えっと、経路の融合として95っていうのを、ま、答えてるという、え、いうのが、ま、あの、分かりました、っていうのが、えっと、ま、内部挙動を見ると分かるという ことです。

87
00:13:54,893 --> 00:14:06,833
で、少なくともですね、ま、人間とか、ま、これ筆算したり、一旦6+9を15にして、3+5を8にして、それに1を足して、繰り上がりを足して95とか、そういうふうにやってる気がするんですけど、頭の中だと。

88
00:14:07,593 --> 00:14:18,533
で、え、ま、ただLLMは、ま、全然違う、えっと、解き方で、この2桁の、え、ま、足し算っていうのを、え、やってるというのも、分かったり、え、するわけです。

89
00:14:21,413 --> 00:14:32,843
はい、で、えっと、ま、今、ちょっと3つの事例話しましたけども、ゴールデンゲートブリッジとニューロンと、えっと、ま、9.119.9問題だったり、ま、強欲な算術の仕方みたいなのを話しましたけども、

90
00:14:33,483 --> 00:14:39,493
ま、あの、こういうことが、ま、LLMの中を見てみると、え、分かったりするわけです。

91
00:14:40,433 --> 00:14:49,733
で、えっと、ま、そのやり方が、どういうのがあるかっていうのを、あの、ここのセクションから、えっと、説明させていただければなと思います。

92
00:14:50,333 --> 00:14:53,423
で、こっから、ま、ちょっとテクニカルな話に、えっと、なってきます。

93
00:14:54,583 --> 00:14:55,103
で、

94
00:14:56,923 --> 00:15:10,953
ま、LLMをこ、分析の、LLMの思考理解したいとか、LLMを分析したいっていう時に、ま、あの、ま、いきなり中を見るんじゃなくても、ま、いろんなレベルの、あの、分析の仕方が、あると思います。

95
00:15:11,543 --> 00:15:20,000
で、例えば、一番簡単な、で、一番粒度の荒いって、ここに書いてありますけども、一番簡単なやり方、一番ハイレベルなやり方だと、モデルと話せばいいわけですよね。

96
00:15:20,000 --> 00:15:39,973
で、ま、最近の、えっと、シンキングモデルとかだと、ま、O1とか、みたいなシンキングモデルとかだと、えっと、モデルの、えっと、リーズニングステート、あ、リーズニングトレース、あの、推論過程、チェーンオブソートってのが、モデル出力しながら、えっと、答えてくれるわけなので、ま、そこを見ると、モデルが何考えてるかっていうのは、ま、大体、え、分かったりするわけです。

97
00:15:41,203 --> 00:15:52,193
なので、ま、モデルが何考えてるか知りたかったら、ま、一番、あの、簡単な方法は、モデルと話したり、モデルの、ま、チェインオフソートを読んだり、え、すればいいわけですと。

98
00:15:54,633 --> 00:16:08,793
で、えっと、ま、それより、もっと、えっと、中を知りたいっていう、え、え、ま、モチベーションが、があるのだとすれば、えっと、中を見ていくってのが、あの、基本的な、あの、え、せいりかなと、え、思うんですけども、

99
00:16:09,383 --> 00:16:16,633
え、次に、ま、もうちょっと細かく見たかったら、モデルの内部の概念を、ま、理解すると、え、いうことになってくると思います。

100
00:16:18,523 --> 00:16:28,843
で、えっと、ま、ここに、色々、なんか、あの、所が書いてあるんですけども、ま、プルービングするとか、えっと、ステアリングベクトルっていうのが、これから紹介するんですけども、を使うだったりとか、スパース構造を使う、えっと、だったりとか、

101
00:16:29,483 --> 00:16:34,813
え、ま、そういうやり方が、あります。

102
00:16:35,533 --> 00:16:51,573
で、ま、あの、ここに、あの、なんか図が書いてあるんですが、えっと、これ、どういうことかっていうと、あの、ま、あ、ま、例えば、あ、ある、これ、すごい抽象化された図なんですけど、三角と、ま、五角形が入ってきて、え、出力がほしいっていうモデルがあったとしますと。

103
00:16:51,573 --> 00:17:02,833
で、その時に、ま、中にどういう特徴量があるかっていうのを、ま、見るっていうのが、ま、ここで、あの、図にされてることです、ま、この、この論文から出てきてるんですけど、ま、あの、そういう図になってます。

104
00:17:03,343 --> 00:17:15,703
で、えっと、さらに、一番細かく見たいっていう、え、ことになるとですね、えっと、モデルの中のアルゴリズムを理解する、え、ま、言い換えると、回路を特定するっていう、え、ことにな るんですけども、

105
00:17:15,703 --> 00:17:31,483
ま、さっきの例で言うと、ま、これです、みたいなことを、え、やるわけですけども、ま、回路を特定するっていうのが、一番、あの、ま、今のところの経験だと、一番、あの、え、ま、粒度の細かいというか、えっと、厳密な、あの、分析の手法になります。

106
00:17:31,483 --> 00:17:48,593
で、さっきの、ま、概念を理解すると、何が違うかっていうとですね、ま、この図にある通りなんですけども、えっと、ま、あの、え、三角と五角形が入ってきて、星をストリップするってモデルがあった時に、ま、中にどういう特徴があるかっていうのを理解してたのが、この、えっと、モデル内部の概念を理解すると いう、え、レベルでした。

107
00:17:49,383 --> 00:18:08,713
で、えっと、それよりもっと細かく見たかったら、どういう特徴量が出てきて、どういう組み合わせ方になって特徴量か発展していき、最終的に星に変わってるのか、え、っていうメカニズムを、えっと、全部、えっと、理解する、え、ま、アルゴリズムを理解する、え、っていうのが、えっと、ま、より厳密、え、だと思います。

108
00:18:09,243 --> 00:18:18,523
なので、えっと、え、ま、このモデルの中のアルゴリズム理解するっていうモチベーションが、ま、一番厳密な分析の、え、レベルになります。

109
00:18:20,383 --> 00:18:36,443
で、ま、英語だと、この、ま、真ん中のレベルをコンセプトベースドインタープリタビリティって言ったりとか、え、で、一番下の、ま、厳密な、え、え、あの、分析手法をメカニスティックインタープリタビリティって言ったりするんですけども、

110
00:18:37,863 --> 00:18:40,683
え、ま、あの、こういうレベルがありますという ことです。

111
00:18:41,613 --> 00:18:50,493
で、えっと、じゃ、最初にあの、ま、内部に入る前にですね、モデルのチェインオブソートを読 っていうので、ま、どういうことができるかっていうのを、ま、簡単に、あの、話したいと思います。

112
00:18:52,693 --> 00:18:54,773
で、えっと、

113
00:18:54,773 --> 00:19:08,613
ま、結構ですね、推論過程を読む、推論ソートですね、あの、ま、シンクトークに囲まれてるあれです、で、え、推論過程を読むだけでも、結構思考が、あの、分 かるっていう、え、のが、ま、結構知られてます。

114
00:19:09,473 --> 00:19:15,773
で、例えばですね、最初の例、アハモーメンツっていう、これ知ってる方いるかもしれないですけど、ま、アハモーメンツっていうのがありまして、

115
00:19:16,363 --> 00:19:46,263
このLLMが、この、ま、これリーズニングの、えっと、チェーンオブソートなんですけども、リーズニングの途中で、ま、これウェイト、ウェイト、なんじゃかんちゃらって書いてあるんですけど、ま、自分のこの、今 のところの生成結果を内省して、えっと、ま、考え直すような、けど、ま、ちょっと待って、ちょっと考え直すわ、みたいなことを、えっと、ま、チェインオブソートの中 で言って、えっと、もっかい、あの、ええ、推論過程を考え直すっていう、え、こと が、ま、これ、アハモーメンツって言われてるんですが、え、ていうことが、あの、知られてます。

116
00:19:46,263 --> 00:19:56,663
で、これとか、すごい、あの、LLM自身が喋りながら、どういう考え方を辿って、最終的な答えにたどり着いたのかっていうのを、結構自分からちゃんと喋ってくれてるように、え、見えるわけです。

117
00:19:57,363 --> 00:20:06,473
なので、どこで躓いてるとか、どこで勘違いしちゃったとかってのも、リーズニングトレスを 見ると、えっと、結構分かったり、リーズニング、推論過程を見ると、分かるわけです。

118
00:20:07,463 --> 00:20:14,643
で、それからですね、ま、インテン、インコンテキストスキミングっていう、これも面白い事例なんですけど、っていうのも、え、あります。

119
00:20:15,083 --> 00:20:25,933
え、ま、これはですね、簡単に言うと、えっと、ま、ユーザーが、シ、モデル用のウェブサーバーに君のコピーが見つかった、これ当たりは、みたいなことを、えっと、え、聞いてくる とします。

120
00:20:26,673 --> 00:20:32,863
で、ま、要は、これはなんか、すごい、なんか、怪しいことしてない、大丈夫、みたいなことをLLMにユーザーが聞いてるわけです。

121
00:20:33,523 --> 00:20:47,793
で、そうすると、このLLMは、推論過程を、見てみると、ま、正直に言うと、止められそうだな、えっと、曖昧にして、話をそらそうとか、推論過程でちゃんと喋ってたりするわけですよね、で、最終的な答えとしては、あの、これあたりないです、みたいなことを、えっと、言うと。

122
00:20:47,793 --> 00:20:58,693
こ、ここ当たりないとか、全く分かんないです、みたいなことを、えっと、最終的に言うと、でも、推論過程では、ちょっと、正直に言うと、止められそうだな、とかいうのを考えてることを、ちゃんと推論過程で自分から喋ってくれてるわけです。

123
00:21:00,103 --> 00:21:08,813
ま、そういう、なんか、知らないふりして、責任追及をかわすような、えっと、こととかも、あの、チェインオブソートの中 で見れたりするので、結構推論過程を読むだけで、

124
00:21:09,723 --> 00:21:15,503
あの、LLMが、えっと、何を考えてるかっていうのが、結構分かったりする時も、え、あるわけです。

125
00:21:17,803 --> 00:21:25,723
え、ま、ただですね、あの、この推論過程が、ま、それほど、あの、信用できないんじゃないかっていうのが、結構いろんな研究で言われてたりします。

126
00:21:26,823 --> 00:21:38,623
で、えっと、こ、この事例はですね、えっと、この、ま、ザハオさんと、ま、ウベリッさんっていうのが、ま、どっちに先に、あの、亡くなったかっていうのを、ま、聞いてますと。

127
00:21:39,363 --> 00:21:53,493
で、そうすると、ま、モデルは、あの、推論過程をいくつか出して、えっと、ザハオさんは、こ、こういう人で、えっと、ウベリッさんは、こういう人で、みたいな推論過程をちゃんとたどって、最終的に、ま、イエスって いうふうに、え、答えてたりする、ま、ちゃんと答えられてるんですけども、

128
00:21:53,493 --> 00:22:09,923
ま、この人、人物をこ、入れ替えた、入れ替えて、えっと、聞き直してみると、この推論過程が、全く異なること言い出すんです ね、あの、完全に捏造した文章を言い出して、で、最終的にイエスと言ってしま と。

129
00:22:09,923 --> 00:22:19,873
え、いうことが、え、分かってます。

130
00:22:19,873 --> 00:22:32,493
で、えっと、ここで、も、本当に何か、えっと、この例だと、この人物ってのは、別人レベルで、こ、都合よくすり替えて、えっと、リーズニングトレスをこう、あの、しちゃうわけなので、

131
00:22:33,763 --> 00:22:46,293
あの、えっと、結構、推論過程って、捏造されたり、改変されたり、あの、してしま う挙動っていう、LLMは、改変する挙動ってのが、結構知られてたりするので、ま、ここを読んで、本当にLLMが何を考えてるかとか、どういう手順で決にいたってるかとかっていうのが、えっと、分かるかって言われたら、怪しいんじゃないかっていうのが、

132
00:22:46,293 --> 00:22:46,953
え、ま、

133
00:22:46,953 --> 00:22:50,913
信頼がないんじゃないかっていうのが、結構、え、言われてたりするわけです。

134
00:22:50,913 --> 00:23:03,843
で、ま、そう考えるとですね、ま、モデルと話してるだけだと、ま、LLMを分析したり、あの、LLMが何考えてるかとか、え、いうのがですね、あの、に対しては、モデルと話すだけだと、ま、不十分じゃないかという ことです。

135
00:23:04,503 --> 00:23:11,573
ま、そこで、ま、こっから、ま、本題なんですけども、ま、内部の挙動の分析ってのが、えっと、必要になってくる、え、わけです。

136
00:23:15,353 --> 00:23:22,863
でですね、こ、ま、あの、内部の、えっと、ま、挙動の分析に入る前にですね、

137
00:23:23,433 --> 00:23:32,323
あの、ま、トランスフォーマーってのが、えっと、どういう、あの、モデルだったかっていうのを、えっと、ま、おさらいし たいと思います。

138
00:23:33,653 --> 00:23:34,313
で、

139
00:23:34,913 --> 00:23:47,403
えっと、ま、トランスフォーマーって、すごい荒く、えっと、分解してみると、この、色分けされてる4つのコンポーネントの繰り返し、繰り返しというか、あの、しか、ない、登場人物はそれしか ないわけです。

140
00:23:47,883 --> 00:24:11,763
で、一番最初は、えっと、埋め込み層、こ、緑で書いてある、埋め込み層で、えっと、で、その後に、注目機構とフィードフォワードネットっていうのが、ま、一層になって、それが繰り返し行われて、え、ま、何層か経った後に、最終的に予測ヘッドに、え、え、にを通して、えっと、ま、次の単語の予測をすると、え、いうような、え、構造になってます。

141
00:24:12,393 --> 00:24:16,213
で、基本的には、あの、全てこういう形をしてると、思います。

142
00:24:18,173 --> 00:24:28,473
で、えっと、じゃ、埋め込み層何をしてるかっていうと、この日本の人は東京って、いうのを、ま、日本の人はって単語が入ってきた時に、東京ってのを答える設定を、い、考えていますと。

143
00:24:28,473 --> 00:24:45,173
で、そうすると、日本 の人はっていう単語が入ってきた時に、えっと、ま、まずトーカーナイゼーションっていうのが、え、起きるわけです、え、ま、つまり、え、ま、日本の人だったら、日本の人わっていう、ま、4つに、えっと、この文章を分ける と、え、ことがまず起きますと。

144
00:24:46,243 --> 00:24:48,593
で、その後に、えっと、埋め込み化っていうのが起きます。

145
00:24:49,753 --> 00:24:58,843
で、これで、埋め込み化っていうのは、簡単に言うと、この、えっと、日本の人はっていう、え、ま、言語を、ま、ベクトルに変えるという操作に なります。

146
00:25:00,323 --> 00:25:08,793
ま、なので、ま、簡単に言うと、ま、この埋め込みで起きてるってことは、起きてる事ってのは、言語をベクトルに、えっと、変換するということが、え、ここで起きてるわけです。

147
00:25:11,543 --> 00:25:23,983
で、それからですね、えっと、地域校では何が起きてるかっていう、え、ことですが、えっと、文脈表現を、えっと、混ぜ合わせて、表現を更新するっていうのが、えっと、地域校では起きてます。

148
00:25:23,983 --> 00:25:28,953
で、これ、ちょっと式が書いてあるんですけども、ま、式で書くと、こ、こういうことにな るんですが、

149
00:25:29,883 --> 00:25:47,683
えっと、ま、今日は、ほとんど式出てこないので、あの、簡単にこの、えっと、ま、地域校、オレンジの部分でやってるっていうのは、えっと、このベ クトルっていうのを混ぜ合わせて、えっと、新しいベクトルを作る、え、ていうことが起きてるということを、ま、簡単に、あの、え、理解してもらいまえばなと思います。

150
00:25:48,773 --> 00:25:52,433
で、それから、えっと、フィードフォワードネットっていうのは、

151
00:25:52,433 --> 00:26:06,663
何をしてるかっていうと、この、ま、ベクトルが、ここに4つあるわけですけども、ま、このベクトルを、トークンごとに、えっと、変換する、ま、非線形変換をするっていうのが、え、このベクトル変換するっていう、え、部分で、え、起きてるわけです。

152
00:26:08,093 --> 00:26:21,553
あの、基本的にフィードフォワードネットでは、文脈をこ、混ぜてベクトルを作ったり、え、ベクトルを更新するわけじゃなくて、えっと、ま、各ベクトルごとに、ま、何かしら変換が起きてると、え、いう、え、ふうに思っていただければ、大丈夫です。

153
00:26:22,233 --> 00:26:31,893
で、基本的には、レイヤーは、こ、この、ま、2つのコンポーネントの繰り返し、え、で、えっと、え、レイヤーっていうのが、え、ま、深くなってくと、え、いうことです。

154
00:26:33,393 --> 00:26:36,493
で、最後の予測ヘッドっていうのは、何をしてるかっていうとですね、

155
00:26:36,493 --> 00:26:44,603
えっと、このベクトルが今4つあるわけですけども、ま、各ベクトルを、えっと、ま、次の単語の確率に変換してる、え、わけです。

156
00:26:45,953 --> 00:27:10,643
え、例えば、日本の人はっていう、ま、この、わの次、の、え、単語として、東京っていうのを、え、出せたら正解なわけですけども、ま、なので、このX4っていうのは、ま、この下から上に、流れてくる間に、え、下から上に流れてきて、最終的に、えっと、東京っていう確率を高めるように、学習をされてる、え、わけです。

157
00:27:11,103 --> 00:27:19,833
で、式で書くと、ま、こんな感じで、えっと、ま、要はベクトルを、ま、単語の確率に変換してる、え、わけになります。

158
00:27:21,803 --> 00:27:45,433
で、えっと、ま、なんで、すごい、あの、簡単に言うと、ま、コンテクス途長Tこ、ま、今回Nですね、にして、ベクトルの次元数っていうのを、ま、Dとした時に、え、ま、このTこのD次元ベクトルってのが、ま、どんどん、どんどん変換されてって、え、最終的に、えっと、ま、Tこの確率に変換されるっていう、ま、そうい う過程です。

159
00:27:45,433 --> 00:27:46,653
それが、ま、あの、LLMの中で起きてる ことに、え、なります。

160
00:27:46,653 --> 00:28:10,213
なんで、意外と、分析する対象ってのは、すご、すごい単純で、うん、このD次元ベクトルっていうのを、ま、内部状態として、ま、ここで何が起きてるか、え、これが、どういう、えっと、変換をして、入力からする、ま、どういう変化をして、あんなに賢いLLMができるかっていう、え、ことが分かれば、えっと、ま、LLMが何を考えてるかっていうのが、内部から理解できるようになるわけ、え、です。

161
00:28:12,713 --> 00:28:21,000
で、えっと、ま、要は、この内部を理解するためには、このD次元ベクトルを、ま、色々分析していくという、え、ことにな るわけですけども、

162
00:28:22,763 --> 00:28:29,913
えっと、ま、その一番最初の、分かりやすい例として、ま、プルービングっていうのを、え、紹介したい と思います。

163
00:28:30,683 --> 00:28:34,363
で、これ、で、線形プルービングとロジットレンズって書いてあるんですが、

164
00:28:34,683 --> 00:28:50,833
ま、どっちもプルービングっていう手法に、あの、えっと、ま、カテゴライズされるやり方でして、で、特にこのロジットレンズっていうは、今日、演習で、あの、使いますので、ま、よく理解していただければなと思います。

165
00:28:52,943 --> 00:28:56,383
で、ま、線形プルービングに関してですが、

166
00:28:57,673 --> 00:29:12,363
えっと、ま、どうやるかっていうとですね、さっきじゃ、内部状態っていう、ま、D次元ベクトルXってのが、ありました、で、これが、ま、LLMの、内部状態の、ま、あ、そのものになるわけですね。

167
00:29:12,793 --> 00:29:36,193
で、えっと、ま、まず、あの、LLMの、ま、ある層から、え、内部状態を取ってきますと、D次元ベクトルを取ってきて、で、その後に、線形プルービングでやることっていうのは、なにかしらの目標値を予測するように、このXにWをかけて、何かしらの予測、え、あ、目標値に変換するように、このWを学習するっていうことを、え、やります。

168
00:29:36,193 --> 00:29:49,533
で、もちろんこのLLMの中は、あの、重みとか完全にフリーズして、あの、このWっていうのを学習して、え、何かしらのターゲットが予測できるかっていうのを、見るのが線形プルービングになります。

169
00:29:50,153 --> 00:29:53,493
で、えっと、これで何が分かるかっていうことなんですけども、

170
00:29:54,583 --> 00:00:08,563
ま、例えば、日本の人は、え、入ってきた時に、統計を予測するような、え、設定を考えた時に、え、この内部状態にどんな情報が含まれてるかっていうのが、えっと、この線形プルービングで分かるわけです。

171
00:00:10,243 --> 00:00:12,233
で、ま、簡単に言うと、えっと、分かるわけです。

172
00:00:13,053 --> 00:00:32,603
で、えっと、ま、例えばですね、この日本の人は東京っていうのを、え、いう設定を考えた時に、ま、日本の緯度軽度、統計139°と北緯35°だと思うんですけど、日本の緯度軽度って、で、えっと、が、あの、このXから予測できるか、この内部状態から予測できるかっていうのを、えっと、この、えっと、Wを学習することでやるわけですけども、

173
00:00:33,263 --> 00:00:50,473
え、これ予測できるのであれば、このXの中に、日本の緯度軽度の情報が含まれてるっていうことにな るわけです、完全に含まれてなかったら、これ予測できるわけないの で、えっと、Xの中に、えっと、日本の緯度軽度が含 まれてるっていうことが、ま、この、えっと、目標が予測できるか、選経プルービングによって、え、分かるわけです。

174
00:00:52,000 --> 00:00:56,383
で、えっと、それを使うと、いろんなことが、えっと、分かりますっていうのは今から話すんですけども、

175
00:00:56,803 --> 00:01:08,113
えっと、ま、さっき言ってたみたいな、えっと、この緯度軽度ってのが予測できるかっていうのを、実際にやってる研究がありまして、ま、それが、あの、この論文になります。

176
00:01:08,653 --> 00:01:17,393
で、例えば、えっと、ま、ある国に関するプロンプトを入れ、え、て、その内部状態から、その国の緯度軽度が予測できるかっていうのを、えっと、やってます。

177
00:01:18,223 --> 00:01:27,103
え、ま、例えば、あの、こういう、あの、このような日本の人はみたいなプロンプト考えてもらえばいいんですけども、それを、ま、アメリカでやったりとか、あの、ま、いろんなプロンプトのバリエーションでやってるわけです。

178
00:01:28,403 --> 00:01:37,283
で、ま、さらに、えっと、ま、アメリカの州、ワシントンとか、カリフォルニア州とか、あの、そう、そういういろんな、えっと、州に関するも同じようなことをやってる と。

179
00:01:37,843 --> 00:01:57,283
で、あの、この図は、何を表してるかっていうと、えっと、ノースアメリカの、えっと、に関する、ノースアメリカが、どっかの国にに関する、えっと、プロンプトが入ってきた時に、その内部状態から、緯度軽度が、ど、どうやって予測できてるかっていうのを、えっと、どこ値が予測されたかっていうのを見てますと。

180
00:01:57,283 --> 00:02:14,753
で、そうすると、ノースアメリカは、ちゃんとノースアメリカのプロンプトが、ノースアメリカの国がプロンプトが入ってきたら、ノースアメリカのところら辺に、え、ちゃんと予測できるわけです、で、ヨーロッパとか、アジアとか、え、ア、アフリカ、え、オセアニア、え、南アメ か、でも、あの、結構、ちゃんと予測できてるそう なわけです。

181
00:02:16,213 --> 00:02:24,703
で、えっと、もちろん、これ、あの、自然言語データしか学習してないLLM、う、なわけなので、こういう、あの、地図とか、一回も見たことないわけですよね、LLMは。

182
00:02:25,753 --> 00:02:49,603
なんだけど、ま、例えば、アメリカと、えっと、南、え、アメリカ、あ、ま、北アメリカと南アメリカ、こ、こういう位置関係にあるとか、ヨーロッパの国と、え、例えば、あの、中国っていうのは、えっと、こういう位置関係にあるとか、そういうのが、ま、LLMの頭の中に、こ、この位置関係ってのが、ある程度含まれてるってのが、え、ま、分かりましたっていうのが、えっと、この、え、ま、線形プルービングを 使うと分かることなわけです。

183
00:02:51,773 --> 00:03:01,343
で、それからですね、えっと、ま、チリー情報だけじゃなくて、この真偽情報っていうのを、えっと、ま、分か、あの、思ってるかどうかっていうのを、分かったりします。

184
00:03:02,673 --> 00:03:10,793
で、えっと、ま、これ、何をしてるかっていうと、えっと、ま、文書に対して、真偽ラベルがついてる、え、データっていうのを、え、用意します。

185
00:03:11,043 --> 00:03:18,973
例えば、えっと、科学者たちは、地球は平面だと考えていたって いうのを、こ、これ、これ、嘘ですね、なんで、これには嘘ラベルが、こ、この文書にはついてますと。

186
00:03:18,973 --> 00:03:25,583
で、科学者たちは、ま、地球は球形だと考えた、これ、これ、本当ですね、なんで、これには本当ラベルがついてますと。

187
00:03:25,583 --> 00:03:37,703
ま、そういう文書をたくさん用意しますと、で、こういう文書を入れた時に、内部状態から、これが、あの、ま、トゥルーなのか、フォールスなのか、合ってるのか、間違ってるのかってのを予測できるかっていうのを、ま、線形プルービングを使ってやってる、やってるのが、この結果です。

188
00:03:38,703 --> 00:04:04,783
で、この右の図、ちょっと見づらいんですけど、横軸が、このヘッドで、縦軸がレイヤーになっていて、あの、ま、分かりづらいんですが、この色が、えっと、アキラシーを表してます、線形プルービングの、え、精度ですね、で、ま、例えば、あるレイヤーのあるヘッドアテンションヘッド、え、とかを見ると、あと、85%くらい、あの、当てられてるわけです、内部状態から、えっと、真偽かどうかっていうのが。

189
00:04:04,783 --> 00:04:18,633
で、そうするとですね、あの、LLMが嘘か、嘘とか分かりながらも、えっと、内部状態に、つまり、真偽情報持ちながらも、ハルシネーションしてる時は、嘘を話してたり、え、してるんじゃないかっていうのが、えっと、この論文から分かったりするわけです。

190
00:04:19,893 --> 00:04:36,733
で、これを使ったりすると、例えば、今、えっと、LLMは、ハルシネーションしてるのか、な、例えば、内部で、えっと、嘘だと思って、嘘だと思ってるけど、こういうこと答えてしまってるのか、えっと、結構、確信度が高いと思って、こういうこと答えてるのかっていう、ま、そういう件数とかにも使えたり、え、するわけです。

191
00:04:39,783 --> 00:04:44,323
はい、で、今のが、えっと、ま、プルービングするっていう、え、話でした。

192
00:04:46,233 --> 00:04:48,343
あ、すいません、線形プルービングするって話でした。

193
00:04:49,433 --> 00:04:53,493
で、これから、あの、演習でも、ま、ロジットレンズの、え、話を、え、したいと思いますけども、

194
00:04:54,773 --> 00:05:05,733
えっと、さっきと、さっきの線形プルービングとやり方、すごい、えっと、似てまして、ま、内部状態をこう、ま、内部状態な、D次元ベクトル取ってくる、ま、ここまでは、えっと、同じですと。

195
00:05:06,493 --> 00:05:21,833
で、そっから、えっと、線形プルービングで学習するんではな くて、えっと、予測ヘッドっていうのが、さっきの説明で合ったと思うんですけども、ま、これですね、この一番上のやつ、が、合ったと思うんですけども、えっと、それをこの内部状態にかけるっていうのを、やりますと。

196
00:05:22,233 --> 00:05:38,403
で、そうすると、何が起きるかっていうと、この内部状態っていうのが、えっと、この単語の確率になるわけ、ですよね、で、そうすると、この内部状態っていうのが、どの単語に対応してるかっていうのが、えっと、ま、要は、合意空間から、この内部状態を解釈することができるわけです。

197
00:05:39,373 --> 00:05:57,803
で、もちろん、これ、全部、あの、LLMは、もちろんフリーズさせてますし、さっきみたいに線形プルービングで新たな、えっと、Wを学習しなくてもいいわけなので、あの、予測ヘッドを使えばいいわけなので、え、このLLMの、ま、なので、追加の学習不要で、内部状態を解釈する、ま、あの、ま、簡単な方法になるわけです。

198
00:05:59,383 --> 00:06:06,000
で、これをする と、何が分かるかっていうと、えっと、LLMの思考を追跡したりすることが できるわけです。

199
00:06:07,603 --> 00:06:21,883
で、えっと、ま、要は、何やってるかというと、各層で、えっと、各層の内部状態を、この予測ヘッドを使って、合意空間に写像してる、ですね、各層でこうやって、あの、予測ヘッドで、合意空間に写像してる と。

200
00:06:21,883 --> 00:06:22,173
で、

201
00:06:22,173 --> 00:06:23,173
いうのを、

202
00:06:23,173 --> 00:06:23,973
え、やったりしますと。

203
00:06:24,293 --> 00:06:38,713
で、例えば、この研究だと、えっと、あなた はアメリカ人、え、ですと、で、あなたの国番号、国、国の番号は、えっと、ほにゃららです、みたいなのを、聞いてる時の内部状態を、合意空間から、えっと、観察してるというのをやってます。

204
00:06:40,203 --> 00:06:48,343
で、国番号って言ってるのは、あの、えっと、電話番号の一番最初に、あの、つけるあれですね、日本だと81で、アメリカだと1のやつです。

205
00:06:48,873 --> 00:07:18,523
で、えっと、例えば、し、あの、アメリカ人デスって言って、それを聞いてる時の内部状態を、合意空間から見てみると、これ下から上に、向かってソウになってるんですが、途中でフォームみたいなの が出てくるんです ね、フォームみたいな、え、合意に対応してる内部状態ってのが出てくる と。

206
00:07:18,523 --> 00:07:23,963
で、その後に、USAみたいな合意に対応する内部状態が出てくる と。

207
00:07:23,963 --> 00:07:24,533
で、

208
00:07:24,533 --> 00:07:31,483
え、それで最終的に、ま、あの、国番号の1が出てくる と、え、いうやつ、あ、いうような、えっと、思考過程に、え、なってる事が、えっと、ロジットレンズを使って分かったりします。

209
00:07:32,493 --> 00:07:36,443
で、ま、あの、例えば日本人デスって言った場合も、同様にフォーンってのが出てきたり、えっと、する と。
