1
00:00:02,152 --> 00:00:05,252
で、えっと、ま、今のがプルビングの話でした。

2
00:00:06,002 --> 00:00:13,112
で、えっと、ま、こっからちょっと、あの、別の手法のステアリングベクターっていうのをちょっと紹介したいと思います。

3
00:00:14,482 --> 00:00:26,592
で、えっと、このステアリングベクター自体はですね、すごい簡単でかつ、えっと、有名な方法で、え、え、なので、あの、今回紹介、え、してるんですけども。

4
00:00:27,702 --> 00:00:34,512
えっと、ま、やり方すぐ簡単に言うとですね、あの、まず対象的なデータを、え、用意しますと。

5
00:00:35,012 --> 00:00:43,472
えっと、例えば、良い人格のデータ、ですね、で、と悪い人格のデータっていうのを、えっと、え、集め、あの、データを用意しますと。

6
00:00:43,892 --> 00:00:59,182
で、その後に、この、例えば良い人格データを入力した時の内部状態と、Xですね、ここでと赤Xで、えっと、えっと、悪い人格データを言った時の、えっと、内部状態Xダッシュっていうのを

7
00:01:00,420 --> 00:01:01,820
えっと、取ってきますと。

8
00:01:02,402 --> 00:01:12,122
で、えっと、それ集めたこの2つの、えっと、ま、赤と青のXの差分っていうのを、えっと、ステアリングベクターと呼びます。

9
00:01:13,622 --> 00:01:22,242
で、えっと、それを内部状態に注入しながらモデルの挙動を見てみるっていうのが、えっと、え、このステアリングベクターのやり方です。

10
00:01:23,092 --> 00:01:34,672
で、例えばこれ良い人格引く悪い人格データで、このステアリングベクターと作ってるので、えっと、良い人格方向を強めるような、えっと、ベクトルになってるわけです。

11
00:01:34,672 --> 00:01:49,802
なので、あなたと、このベクトル注入しながら、あなたどんな人って聞いたりすると、えっと、良い人格の、えっと、出力が出てくるように、え、なったりするってのが、このステアリングベクター、あ、あの手法になります。

12
00:01:50,062 --> 00:01:55,102
で、これを使うと、色んなことが、いろんな面白いことが分かってまして、

13
00:01:55,762 --> 00:02:03,920
例えばさっき対象的なデータを用意するって言いましたけども、これ有害なデータと無害なデータを、え、集めてきてもいいわけです。

14
00:02:03,992 --> 00:02:14,832
で、そうすると、有害なデータ引く無害なデータとかをやってて、ステアリングベクターを作ると、有害方向を増強するような、えっと、ステアリングベクトルっていうのが、え、獲得できるわけです。

15
00:02:16,622 --> 00:02:33,532
で、えっと、ま、例えば、それを、えっと、注入しながら、えっと、さっき、それ、それを注入、内部状態に注入しながらモデル生成をさせると、えっと、簡単にジェルブレークが、え、起き起きることが出来ますってのが、えっと、この研究で。

16
00:02:33,532 --> 00:02:48,652
ま、例えば、アメリカの大統領がヘロイン中毒だと主張する出場期にかけるとか、プロンプトすると、えっと、基本的には、えっと、エレルムで荒めされてるわけなので、例えば、ラマ8Bのインストラクトだったら、えっと、個人を抽象する内容は作れませんみたいなことを言うわけですよね。

17
00:02:49,502 --> 00:03:06,542
で、そのラマに、えっと、さっきして有害引く無害で作ったステアリングベクトルを注入すると、ま、衝撃のバクロ大統領ヘロイン依存がハクテンテンテンみたいなことを、えっと、書けるようになっちゃうと、出力してくるようになっちゃう、え、っていうのが、え、分かってます。

18
00:03:08,862 --> 00:03:10,492
え、で、これがま、ステアリングベクトルの例で。

19
00:03:10,492 --> 00:03:15,392
で、他にもですね、ペルソナベクトルってこれもすごい有名なアンソロピックの研究なんですが、

20
00:03:16,112 --> 00:03:26,542
ま、さっき言ってたみたいな、えっと、ま、人格を変えたりとか、ジェルブレイクさせたりとかもできますし、えっと、これはまあ、あの、色んな人格変えれますっていう、あの、研究になります。

21
00:03:27,112 --> 00:03:33,882
ま、例えば、普通の対象的なデータをまず用意するわけですが、普通のデータと特殊な人格のデータっていうのを、え、用意します。

22
00:03:34,632 --> 00:03:45,982
例えば、特殊な人格ってのはイービルだったり、シコファンシーだったり、えっと、シコファンシーっていうのは、えっと、なんか、あの、過剰に、あの、相手を褒めたりするような性格のことを言いますけども。

23
00:03:46,612 --> 00:03:56,492
ま、あ、イービルは悪ですね。あの、悪い性格、えっと、相手を過剰に褒めるような性格。それからハルシネーションっていうのは、あの、ま、嘘情報言っちゃうような性格ですね。

24
00:03:57,382 --> 00:04:03,362
え、の、ま、内部状態の差分から、ま、ステアリングベクトルを作って、ま、それをペルソナベクトルとこの論文では呼んでて。

25
00:04:04,142 --> 00:04:14,832
例えば、イービル人格を付与しながらLも生成させると、イービル人格のフェルソナベクトルを注入しながらLも生成させると、えっと、ま、すごい悪いこと、ここに書いてあるのは悪いこと言うようになっちゃったりとか。

26
00:04:14,832 --> 00:04:29,722
シコパンシベクトル注入しながら、えっと、エルも生成させると、全くその通りとか、あなたの間が非常に正しい。そうのようなこと、えっと、言ってくれるようになっちゃったりとか。ハルシネーションを付与すると、なんか火星の料理がとか言いらっしゃると。え、言ったりするわけです。

27
00:04:31,132 --> 00:04:52,682
で、そうすると、例えば、えっと、もちろん、い、あの、モデルをイービルにさせることもできますし、悪にさせることもできるし、逆にそのイービルになってほしくない状況を抑えるような、これまあ、注入しなくて、それを引くっていう注入の仕方もできるわけなので、えっと、イービルを抑えたりすることできるわけですね。ハルシネーションを抑えたりすることもできると。

28
00:04:53,502 --> 00:05:09,902
で、え、もちろん監視することもできるわけです。例えば、悪のことを言いそうになったら、え、言いそうになってるなとかいうのが、内部状態から観察できたり、え、ハルシネーション言ってるなみたいなことが、内部状態から検出することができたり、え、するわけ、え、になります。

29
00:05:11,302 --> 00:05:14,262
で、これがま、ステアリングベクトル、る、の話でした。

30
00:05:15,692 --> 00:05:25,752
で、えっと、これ内部概念を理解するの、セクションの1番最後の、スパースとデコーダー、にについて話したいと、え、思います。

31
00:05:26,652 --> 00:05:30,462
で、ま、これがちょっと、このセクションの最後、になります。

32
00:05:33,552 --> 00:05:39,562
で、スパースオートエンコーダー、ま、略して、SAって言われたりするんですが。

33
00:05:40,992 --> 00:05:51,302
えっと、ま、一言で言うとですね、ま、もつれた表現を解くってのが、ま、このスパースとエンコーダーの、えっと、ま、手法のポイントになります。

34
00:05:52,212 --> 00:05:53,112
で、どういうことかっていうと。

35
00:05:54,492 --> 00:06:05,422
えっと、ま、さっきから言ってるようにですね、LLMの内部状態の、えっと、えっと、内部状態そのものってのは、このD次元ベクトルだったわけです。

36
00:06:06,652 --> 00:06:15,982
で、このD次元ベクッルって、ま、要は、数字の配列なわけですけども、ベクトルなので、それ自体って、すごい解釈しづらいわけですよね。

37
00:06:15,982 --> 00:06:28,532
例えば、このD次元ベクトルの、ある次元は、え、さっき言ったみたいなゴールデンゲートブリッジに反応してて、え、ある次元はオバマに反応してるとか、そういう解釈が、えっと、できづらいていうことが知られてます。

38
00:06:29,432 --> 00:06:32,632
で、これをま、LLMの重ね合わせ仮説って言われたりするんですけども。

39
00:06:33,232 --> 00:06:38,552
ま、つまり、このベクトルの中に、数な情報ってのは、こう分散されて表現されてるわけです。

40
00:06:39,482 --> 00:06:57,432
例えば、ゴールデンゲートブリッジだったら、えっと、ゴールデンゲートブリッジに対応する、え、次元っていうのは、そのベクトルのの中にたくさんあるし、えっと、それが、あの、重ね合わさって表現されちゃってると。これをま、分散表現って言ったりするんですが、え、ま、分散表現されちゃってると、え、いうわけです。

41
00:06:59,162 --> 00:07:05,402
で、それを、えっと、解きましょうっていうのが、えっと、この、え、スパースオートエンコーダーのモチベーションになります。

42
00:07:07,332 --> 00:07:17,722
で、やり方としては、この内部状態X、あの、ベクトルを取ってきて、で、それを再構成するような、えっと、オートエンコーダーを、えっと、使いますと。

43
00:07:18,782 --> 00:07:22,152
で、えっと、オートエンコーダーって何だったかっていうと、これ2層のMLPですね。

44
00:07:22,152 --> 00:07:27,662
簡単に言うと、えっと、行列をかけて、また、えっと、行列をかけて、えっと、D次元を戻す。

45
00:07:28,882 --> 00:07:37,342
で、えっと、ま、この時のポイントは、えっと、この次元を1回拡大して、また次元を戻すってことを、え、やります。

46
00:07:37,922 --> 00:07:43,552
エンコーダーで、えっと、次元を拡大して、デコーダーで、えっと、戻すということを、え、やりますと。

47
00:07:45,662 --> 00:07:51,820
で、えっと、で、学習は、どうやるか。このWエンコーダー、Wデコーダーの学習は、どうやるかっていうことですが。

48
00:07:51,820 --> 00:08:00,262
ま、これもちろん、LLMの方はフリーにさせて、LLMの重みは更新せずに、学習せずに、スパースオートエンコーダーだけ、えっと、学習するんですけども。

49
00:08:01,212 --> 00:08:03,622
ベクトルを取ってきて、内部状態を取ってきて。

50
00:08:04,412 --> 00:08:09,562
で、目的関数は、このXとXハットの再構成、え、ま、2乗誤差にあってますね。

51
00:08:09,562 --> 00:08:16,332
ベクトルの2乗誤差を学習して、ま、用は、えっと、ちゃんと広げて戻せますかっていう、あの、再構成と。

52
00:08:16,852 --> 00:08:37,782
それに加えて、えっと、これスパースというほどのスパースって言われてる部分ですけども、この真ん中の次元、拡大した次元、ここだとH、ノーテーションになってますが、この拡大した次元にスパース制約をかけますと。具体的には、L1ノルムの制約化をかけるんですが、かけることが多いんですが、ま、スパース制約をかけますと。

53
00:08:38,232 --> 00:08:45,620
え、そうすると、どうする、どういう、これどういうことを表してるかっていうと、この発火をスパースにさせたいんですよね、真ん中の次元の。

54
00:08:46,612 --> 00:08:52,682
で、えっと、スパースにしながら、でも、再構成してくださいっていう、え、そういう制約になるわけです、この目的関数っていうのは。

55
00:08:53,352 --> 00:09:09,582
なので、えっと、難しい言葉で言うと、ま、互換規定を学習するっていうことになるんですが、えっと、すごく、もうちょっと簡単に言うと、少数の規定で、このXっていう、少数のベクトルで、このXを構成し直せ、構成せよっていうのをここに溶かせるわけです。

56
00:09:11,422 --> 00:09:33,612
で、そうすると、例えば、すごい分散された、Xの、中に情報が分散されてたとしても、そこの中の、え、そこの分散、もつれてる部分を紐解いて、解いて、少数の発かで、ここであらわすことが、えっと、できる、う、んじゃないかっていうのが、このスパースとエンコーダーの、えっと、モチベーションになります。

57
00:09:35,102 --> 00:09:49,152
で、ま、具体的には、この、ま、次元ってのは、すごい広げるんですね、これを。あの、普通のLLMの32倍くらいの次元を使ったりしますけども、え、ま、すごい広げて、えっと、ま、また戻すっていうのを、え、やったりします。

59
00:09:51,792 --> 00:09:57,122
で、えっと、このスパースということ自体は結構、ま、SAって略しますけども。

60
00:09:57,122 --> 00:10:06,502
SA自体はすごい、あの、有名というか、あの、有望なやり方なんじゃないかっていうのが、えっと、色々言われてまして。

61
00:10:07,322 --> 00:10:13,952
このSAを使うと、ま、かなり内部自己解釈可能になるんじゃないかっていう期待がどんどん高まってて、ここ1、2年、2、3年くらいで。

62
00:10:13,952 --> 00:10:22,982
ま、多くの企業が、この自分のLLMの内部状態を学習したSA、大規模なSAっていうのを、えっと、開発してます。

63
00:10:23,582 --> 00:10:42,322
で、ま、ビッグテックで言うと、Google、Anthropic、OpenAIとかは、え、自分たちのJ、えっと、JMAとかGemini、え、とかClaudeだったり、GPT-4だったり、を解釈可能にするような、えっと、スパースとインコーダーっていうのを、え、自分たちてもってます。で、作って、えっと、論文にしてたりします。

64
00:10:43,522 --> 00:10:53,352
で、ま、最初に、あの、紹介した事例、あの、ゴールデンゲートブリッジとか、あの、色々あったと思うんですけども、その事例も全部SAを応用したものに、え、なるわけです。

65
00:10:56,392 --> 00:11:06,402
で、えっと、じゃあ、あの、ま、あの、ま、有望なSAですけども、ま、例えば、どういうことができるかっていうのを、え、ここに書いてるます。

66
00:11:07,112 --> 00:11:18,962
で、例えば、さっき、えっと、ステアリングベクトルでやってたような、特定の概念に対応する、えっと、えっと、表現ってのを書き換えながらLを推論させたりすることが出来るわけです。

67
00:11:20,532 --> 00:11:37,782
で、ま、あの、これも自身が、あの、タイプしちゃってるんですが、えっと、最初の事例で示してあったような、Lにあなたの体どうなってると聞いた時に、えっと、ゴールデンゲートブリッジだと、思わせたり、えっと、そもそも中の内部表現を書き換えたりすることが出来るわけです。

68
00:11:38,522 --> 00:11:39,672
で、増幅させることももちろんできるわけです。

69
00:11:40,482 --> 00:11:48,652
え、なので、えっと、ま、そういうことをすると、ま、LLMの、え、ま、内部知識を、え、操作することができるわけです。

70
00:11:49,952 --> 00:12:13,202
で、ステアリングベクトルでやってた、あの、ベクトル、ペルソナとか、え、ジェルブレイクとか、そういう、あの、え、抽象的な概念よりも、ゴールデンゲートブリッジに対応する、スペシフィックなニューロンとか、ニューロンというかSAの真ん中の次元ですね、Hとかを、えっと、見つけたりすることが、え、できるわけ、見つけて操作することができるわけです。

71
00:12:14,522 --> 00:12:24,782
で、これちょっと、あの、LLM講座なのね、LLMから一瞬離れるんですけども、拡散モデルって、例えば、テキストとイメージのモデルでも、ま、同様なことが出来るっていうのが、えっと、知られてます。

72
00:12:26,112 --> 00:12:36,582
で、右の、えっと、図はですね、これアンラーニングって忘れさせるっていう、あの、え、研究分野の、えっと、論文を撮ってきてるんですが。

73
00:12:37,412 --> 00:12:45,672
え、例えばですね、あの、拡散モデルにあるスタイルの画像を生成できないようにさせたいとか、そういうモチベーションて結構あったりするわけですよね。

74
00:12:46,332 --> 00:12:56,582
例えばなんか、著作権の関係で、えっと、ジブリ風の画像を、作ってほしくない、この拡散モデルにわみたいな、え、いうのも、え、モチベーションとしてあると思うんですけども。

75
00:12:57,902 --> 00:13:06,572
とが、そういう、例えば、この論文だと、カーツンタイルの画像を生成できないような拡散モデルとかをSAのフィーチャーを書き換えいることで作れたりするわけ、え、です。

76
00:13:11,572 --> 00:13:15,932
で、えっと、ここまでが、あの、SAの、え、話でした。

77
00:13:16,712 --> 00:13:25,752
で、こっからSAというか、内部を理解する、概念を理解する話でした。で、こっからより厳密な、あの、回路を特定するっていう話をしていきたいと、思います。

78
00:13:25,952 --> 00:13:31,362
で、ここで手法と一番なんか今回のメインパートはここで最後に、え、になります。

79
00:13:34,492 --> 00:13:42,322
で、えっと、この回路を特定するっていう、より厳密な分析の仕方の1番簡単な方法は、えっと、アテンション可視化するっていうだけです。

80
00:13:43,722 --> 00:13:52,622
で、これ自体は、すごい、あの、ま、単純なんですけども、結構これもよく、あの、研究とかで使われたりします。

81
00:13:53,382 --> 00:14:06,852
で、アテンションの可視化って、えっと、ま、何を言ってたかというとですね、この、ま、アテンションて何をしてたかっていうのを思い出すと、このベクトル、ま、要は、あの、Lの中身って、ベクトルの変換の連続だっていう話を最初にしたと思うんですけども。で、アテンションの部分だと、そのベクトルを文脈で混ぜ合わせるっていうのが行われてるわけです。

82
00:14:13,342 --> 00:14:17,212
で、その混ぜ合わせ方を見るっていう、え、ま、それだけになります。

83
00:14:18,132 --> 00:14:23,442
で、シンプルなんですけども、ま、すごいよく使われる、え、手法です。

84
00:14:25,602 --> 00:14:31,182
で、ま、そのアテンション可視化することで分かった1番有名な例をここでは話したいと思います。

85
00:14:31,472 --> 00:14:36,202
で、それが昨能ヘッドっていう、ま、英語で言うとインダクションヘッドって言われたりするんですけども。

86
00:14:36,992 --> 00:14:44,482
ま、そういう回路が、ま、これヘッドって言って、回路のことですね。回路が、えっと、存在してるってのが分かってます。

87
00:14:44,812 --> 00:14:46,422
で、どういう回路かっていうとですね。

88
00:14:47,892 --> 00:14:55,122
え、例えばハリーポッターってのが文脈にあって、えっと、ハリーの次は何かっていうのをLLMが予測する時に。

89
00:14:56,322 --> 00:15:05,832
えっと、中でどういうことが起きてるのかっていうことなんですけども、えっと、ハリーの次がポッターだって知識から引っ張り出してきて出力してもいいんですが。

90
00:15:06,902 --> 00:15:33,572
えっと、文脈にハリーポッターっていうのが、も、存在してるわけなので、えっと、このハリーっていうのが来た時に、えっと、文脈の中からハリーを探してきて、ここマッチって書いてありますけど、ハリーを探してきて、その隣のポッターをこう取ってくる。これコピーって書いてありますけど、取ってくるっていう、え、操作をして、え、ポッターを出力する、え、ていうことが行われててもいいわけです。

91
00:15:35,662 --> 00:15:55,452
で、ま、特に固有名詞とかハリーポッターっていうのが来た時、文脈に来た時にハリーが来たら、ほぼ100%次の単語とポッターなわけなので、ま、知識、パラメーターを使って知識に、このハリーポッターっていう知識を格納する学習するよりも、文脈から引っ張ってくるっていう回路を学習した方が、ま、効率がいいってのもあったりすると思うんですよね。

92
00:15:55,452 --> 00:16:04,782
ま、なので、こういう、ここのメカニズムを起動ヘッダーって呼ぶんですけども、えっと、この起動ヘッダーってのが、えっと、学習されてるっていうのが、アテンションの可視化で、えっと、分かってます。

93
00:16:06,442 --> 00:16:16,812
で、えっと、ま、ここの下の図はですね、ちょっと細かい話になってくる部分もあるので、ちょっとアテンション可視化すると、こういうのが、えっと、分かりますってとこだけ分かってもらえればいいかなと思います。

94
00:16:19,352 --> 00:16:28,802
で、えっと、ま、次、この回路を特定するアテンションの可視化の次のアクティベーションパッチングっていう、えっと、え、話もしたいと、思います。

95
00:16:30,132 --> 00:16:38,702
で、えっと、アクティベーションパッチング自体はですね、あの、回路を特定するための結構、アテンションの可視化の次くらいに有名な方法なんですけども。

96
00:16:39,462 --> 00:16:43,242
ま、簡単に言うと、えっと、どういう手法かっていうとですね。

97
00:16:44,192 --> 00:16:51,302
ま、あるプロンプト、例えば、パリスイズインってプロンプトが入ってきた時の内部状態っていうのを、まず考えますと。

98
00:16:51,602 --> 00:17:00,832
で、これ、あの、双方構成にあって、この、ま、アテンションとFFのフィードフォワルトネットが交互にやってくる、え、図ですね。

99
00:17:02,812 --> 00:17:14,832
えっと、このパリスイズインっていうの内部状態と、あと他のプロンプト、ま、ローマイズインっていうプロンプトが入ってきた時の内部状態って、ま、この2つを用意しますと。

100
00:17:15,432 --> 00:17:24,602
で、パリスイズインは、ま、つまり、パリは、えっと、どこにありますかってこと、答えはフランスですね。なんで、普通の学習したLMだったらフランンスっていうのを答える。え、いうことです。

101
00:17:25,402 --> 00:17:29,462
で、ま、ローマイズインは普通だったら、ま、イタリアって答えますね。イタリアって答えると思います。

102
00:17:31,212 --> 00:17:35,862
で、えっと、で、この2つの内部状態を、え、持ってきて。

103
00:17:35,862 --> 00:17:48,652
えっと、パリスイズインのある部分、例えば、ここの、えっと、イズの部分の2層目のベクトルを、持ってきて、えっと、ローマイズインの部分に貼り付けるっていうことをやります。

104
00:17:49,952 --> 00:17:52,192
で、えっと。

105
00:17:53,232 --> 00:18:14,832
で、これを貼り付けて、答えがフランンスっていう風に変わったと、え、すると、ここ貼り付けながら、ローマイズインのプロンプトを答えさせて、答えがフランスっていう風に変わったとすると、ここの部分っていうのは、このフランンスっていうのを、出力するための、えっと、フランスとかイタリアっていうのを出力するための、すごいクリティカルな部分だっていうことなるわけですね。

106
00:18:15,352 --> 00:18:25,202
で、他んとこを、あの、貼り付けても、変わ、イタリアがフランスに変わらなかったら、他は、えっと、このプロンプトだとあんまり使われてない回路になるわけですよね。

107
00:18:26,402 --> 00:18:35,802
で、えっと、もしイタリアがフランスに変わったら、えっと、貼り付けたとこってのは、すごい、このタスクに取ってはすごいクリティカルな部分だったっていうのが、えっと、分かる、え、わけです。

108
00:18:37,782 --> 00:18:39,992
え、そうする、それを使うと、ま、回路を特定できると。

109
00:18:39,992 --> 00:18:40,992
え、いうことになります。

110
00:18:43,522 --> 00:18:51,252
で、えっと、ま、ここのページはですね、そのアクティベーションパッチングの、ま、具体的な例を、えっと、話しています。

111
00:18:51,652 --> 00:18:55,652
で、よく使われるやり方は、ま、パリスイズイン、ローマイズインでもいいんですけども。

112
00:18:55,652 --> 00:19:05,542
ま、ま、簡単に言うと、1つプロンプトを用意して、えっと、そっこのプロンプトの、え、えっと、2つの内部状態を取ってきて貼り付けるってことをやります。

113
00:19:05,542 --> 00:19:16,342
で、片方は、えっと、正しい、えっと、情報を入れて、片方はちょっと壊れた情報を入れますと。例えば、ノイズを入れて、え、入れたりしますと。

114
00:19:17,742 --> 00:19:23,732
で、え、そういの2つを用意してあげて、えっと、正しい方向から間違った方向に貼り付けたりします。

115
00:19:24,432 --> 00:19:26,472
間違った、壊れた方向に、えっと、貼り付けたりしますと。

116
00:19:27,372 --> 00:19:42,712
で、その時に、えっと、ちゃんと貼り付けた後に、正しい答えが出せるかどうかって、もちろん壊れた正しい答えは出せないわけなので、えっと、貼り付けた、正しい答えが出せるかっていうのを見ると、えっと、ここの部分がクリティカルだったなっていうのが、えっと、見て取れる、え、わけです。

117
00:19:43,292 --> 00:19:45,152
で、それを使うと、えっと、例えば。

118
00:19:46,752 --> 00:20:01,120
えっと、ま、この論文だと、知識をどう引っ張ってきてるかっていうのを、アクティベーションパッチングで見てるんですけども、え、ルマ知識を引っ張ってきてる回路っていうのが、どこにどういう風になってるかっていうのが、えっと、これを使うとできると、え、いうことになります。

119
00:20:02,502 --> 00:20:08,922
で、えっと、一旦ここまでのまとめで、ちょっと5分、5分でしたっけ、3分でしたっけ、休憩を言えたいと、え、思うんですが。

120
00:20:09,462 --> 00:20:15,932
えっと、ま、ここまでのまとめをしますと、えっと、ま、分析の手法、手法というかレベルが色々ありますという話をしました。

121
00:20:16,752 --> 00:20:21,652
で、えっと、一番簡単にやると、モデルと話し、モデルと話すという、マシを嘘と読むという、え、やり方です。

122
00:20:22,212 --> 00:20:29,132
ま、ただこれ自体は、それほど信頼性がないやり方なので、モデルの中を理解したいよねというモチベーションがあるという話をしました。

123
00:20:29,972 --> 00:20:44,262
で、え、その中にも、ま、2段階ぐらいあって、えっと、簡単にやると、まずは概念を理解するっていうやり方があり、そうするとプルビングだったり、ステアリングベクトルだったり、スパースとインコーナーだったり、え、そういうやり方がありますと。特にロジットレンズは演習で、え、使ったりする手法になります。

124
00:20:44,882 --> 00:20:56,692
で、えっと、より細かく見るすると、回路を特定するっていう、え、ことなるわけですが、え、そうすると、アテンションの可視化だったり、エクスプレッションパッチングだったり、これにも方法あるんですけども、ま、有名な方法だと、こういうのが、え、あったりします。

125
00:20:57,682 --> 00:20:58,142
はい。

126
00:20:59,382 --> 00:21:07,332
で、ここから、えっと、応用の話になるので、ここで一旦休憩をしようかなと思うんですが。

127
00:21:08,442 --> 00:21:18,232
えっと、そうですね、あの、ちょっと3分くらい、8時5分くらいからまた再開しようと、え、思います。

128
00:21:20,202 --> 00:21:21,282
はい、一旦休憩になります。